{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data-Jobs-Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRDxKmJ68VkJ"
      },
      "source": [
        "# Data Jobs Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTOjU1xF8d_V"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG7MhtZI8g_h"
      },
      "source": [
        "In the field of Big Data, Internet is full of job advertisements by companies looking for different profiles at each level (**Junior**, **Middle**, **Senior**, **Tech lead**...) and different skills to take up a new position (**Data Analyst**, **Data Engineer**, **Data Scientist**, etc).\r\n",
        "\r\n",
        "Our objective is to handle 4 datasets containing thousands of Big Data related job advertisements and build a model which will allow us to predict the **salary** based on a few variables like **industry**, **location**, **company revenue**, **experience**, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Q0bGPEbwokN"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "\r\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HuuJYV7qX9c"
      },
      "source": [
        "df_dataanalyst = pd.read_csv('/content/drive/MyDrive/Kaggle-Datasets/DataAnalyst.csv')\r\n",
        "df_dataengineer = pd.read_csv('/content/drive/MyDrive/Kaggle-Datasets/DataEngineer.csv')\r\n",
        "df_businessanalyst = pd.read_csv('/content/drive/MyDrive/Kaggle-Datasets/BusinessAnalyst.csv')\r\n",
        "df_datascientist = pd.read_csv('/content/drive/MyDrive/Kaggle-Datasets/DataScientist.csv')\r\n",
        "\r\n",
        "df = pd.concat([df_dataanalyst, df_dataengineer, df_businessanalyst, df_datascientist])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "id": "N76KxlKgc1O-",
        "outputId": "b0f973ee-1a74-4cce-fa78-aa8f35c7eb4b"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Job Title</th>\n",
              "      <th>Salary Estimate</th>\n",
              "      <th>Job Description</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Company Name</th>\n",
              "      <th>Location</th>\n",
              "      <th>Headquarters</th>\n",
              "      <th>Size</th>\n",
              "      <th>Founded</th>\n",
              "      <th>Type of ownership</th>\n",
              "      <th>Industry</th>\n",
              "      <th>Sector</th>\n",
              "      <th>Revenue</th>\n",
              "      <th>Competitors</th>\n",
              "      <th>Easy Apply</th>\n",
              "      <th>index</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Data Analyst, Center on Immigration and Justic...</td>\n",
              "      <td>$37K-$66K (Glassdoor est.)</td>\n",
              "      <td>Are you eager to roll up your sleeves and harn...</td>\n",
              "      <td>3.2</td>\n",
              "      <td>Vera Institute of Justice\\n3.2</td>\n",
              "      <td>New York, NY</td>\n",
              "      <td>New York, NY</td>\n",
              "      <td>201 to 500 employees</td>\n",
              "      <td>1961</td>\n",
              "      <td>Nonprofit Organization</td>\n",
              "      <td>Social Assistance</td>\n",
              "      <td>Non-Profit</td>\n",
              "      <td>$100 to $500 million (USD)</td>\n",
              "      <td>-1</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Quality Data Analyst</td>\n",
              "      <td>$37K-$66K (Glassdoor est.)</td>\n",
              "      <td>Overview\\n\\nProvides analytical and technical ...</td>\n",
              "      <td>3.8</td>\n",
              "      <td>Visiting Nurse Service of New York\\n3.8</td>\n",
              "      <td>New York, NY</td>\n",
              "      <td>New York, NY</td>\n",
              "      <td>10000+ employees</td>\n",
              "      <td>1893</td>\n",
              "      <td>Nonprofit Organization</td>\n",
              "      <td>Health Care Services &amp; Hospitals</td>\n",
              "      <td>Health Care</td>\n",
              "      <td>$2 to $5 billion (USD)</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Senior Data Analyst, Insights &amp; Analytics Team...</td>\n",
              "      <td>$37K-$66K (Glassdoor est.)</td>\n",
              "      <td>Weâ€™re looking for a Senior Data Analyst who ha...</td>\n",
              "      <td>3.4</td>\n",
              "      <td>Squarespace\\n3.4</td>\n",
              "      <td>New York, NY</td>\n",
              "      <td>New York, NY</td>\n",
              "      <td>1001 to 5000 employees</td>\n",
              "      <td>2003</td>\n",
              "      <td>Company - Private</td>\n",
              "      <td>Internet</td>\n",
              "      <td>Information Technology</td>\n",
              "      <td>Unknown / Non-Applicable</td>\n",
              "      <td>GoDaddy</td>\n",
              "      <td>-1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Data Analyst</td>\n",
              "      <td>$37K-$66K (Glassdoor est.)</td>\n",
              "      <td>Requisition NumberRR-0001939\\nRemote:Yes\\nWe c...</td>\n",
              "      <td>4.1</td>\n",
              "      <td>Celerity\\n4.1</td>\n",
              "      <td>New York, NY</td>\n",
              "      <td>McLean, VA</td>\n",
              "      <td>201 to 500 employees</td>\n",
              "      <td>2002</td>\n",
              "      <td>Subsidiary or Business Segment</td>\n",
              "      <td>IT Services</td>\n",
              "      <td>Information Technology</td>\n",
              "      <td>$50 to $100 million (USD)</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Reporting Data Analyst</td>\n",
              "      <td>$37K-$66K (Glassdoor est.)</td>\n",
              "      <td>ABOUT FANDUEL GROUP\\n\\nFanDuel Group is a worl...</td>\n",
              "      <td>3.9</td>\n",
              "      <td>FanDuel\\n3.9</td>\n",
              "      <td>New York, NY</td>\n",
              "      <td>New York, NY</td>\n",
              "      <td>501 to 1000 employees</td>\n",
              "      <td>2009</td>\n",
              "      <td>Company - Private</td>\n",
              "      <td>Sports &amp; Recreation</td>\n",
              "      <td>Arts, Entertainment &amp; Recreation</td>\n",
              "      <td>$100 to $500 million (USD)</td>\n",
              "      <td>DraftKings</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Unnamed: 0  ... index\n",
              "0          0  ...   NaN\n",
              "1          1  ...   NaN\n",
              "2          2  ...   NaN\n",
              "3          3  ...   NaN\n",
              "4          4  ...   NaN\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eWmYDe6eUyL",
        "outputId": "1f6c70cf-c580-4487-9bda-ab2c76e45453"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 12782 entries, 0 to 399\n",
            "Data columns (total 17 columns):\n",
            " #   Column             Non-Null Count  Dtype \n",
            "---  ------             --------------  ----- \n",
            " 0   Unnamed: 0         9854 non-null   object\n",
            " 1   Job Title          12782 non-null  object\n",
            " 2   Salary Estimate    12782 non-null  object\n",
            " 3   Job Description    12782 non-null  object\n",
            " 4   Rating             12782 non-null  object\n",
            " 5   Company Name       12781 non-null  object\n",
            " 6   Location           12782 non-null  object\n",
            " 7   Headquarters       12782 non-null  object\n",
            " 8   Size               12782 non-null  object\n",
            " 9   Founded            12782 non-null  object\n",
            " 10  Type of ownership  12782 non-null  object\n",
            " 11  Industry           12782 non-null  object\n",
            " 12  Sector             12782 non-null  object\n",
            " 13  Revenue            12782 non-null  object\n",
            " 14  Competitors        12782 non-null  object\n",
            " 15  Easy Apply         12782 non-null  object\n",
            " 16  index              7601 non-null   object\n",
            "dtypes: object(17)\n",
            "memory usage: 1.8+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlicA-DEYdnw"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWMlmVSte80y"
      },
      "source": [
        "# First, we need to clean the data. We remove the columns we don't need:\r\n",
        "\r\n",
        "def drop_features(df):\r\n",
        "  df.drop(labels=['Unnamed: 0','index'], axis=1, inplace=True)\r\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ55HmVxg19a"
      },
      "source": [
        "# Using the unique method we notice that df_businessanalyst dataset contains values in 'Founded' column which are not years and values which are not ratings in 'Rating' column.\r\n",
        "# In fact, all the values in df_businessanalyst are shifted and placed in a wrong column\r\n",
        "# To fix this problem, we are going to change the column order of these particular rows:\r\n",
        "\r\n",
        "def fix_df_columns(df):\r\n",
        "  sub_df_corrected = pd.DataFrame(df.loc[df['Rating'].astype(str).str.contains('[A-Za-z]') == True, :].to_numpy(), columns=['Job Title', 'Job Description',\r\n",
        "       'Rating', 'Company Name', 'Location', 'Headquarters', 'Size', 'Founded',\r\n",
        "       'Type of ownership', 'Industry', 'Sector', 'Revenue', 'Competitors',\r\n",
        "       'Easy Apply', 'index', 'Unnamed: 0', 'Salary Estimate'])\r\n",
        "  df = df[df['Founded'].astype(str).str.contains('[A-Za-z]') == False]\r\n",
        "  df = df[df['Rating'].astype(str).str.contains('[A-Za-z]') == False]\r\n",
        "  df = pd.concat([df,sub_df_corrected], axis=0)\r\n",
        "  df.reset_index(drop=True, inplace=True)\r\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g36ilABJYGWH"
      },
      "source": [
        "def convert_dtypes(df):\r\n",
        "  df['Rating'] = df['Rating'].astype(float)\r\n",
        "  df['Founded'] = df['Founded'].astype(float)\r\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hq6PCBrn0qmS"
      },
      "source": [
        "def replace_null(df):\r\n",
        "  df.replace({-1: np.nan, -1.0: np.nan, '-1': np.nan, 'Unknown': np.nan}, inplace=True)\r\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v83N4ODb2kf"
      },
      "source": [
        "# We notice that the Company Name column contain a '\\n' after the company name value, followed by a duplication of the company's rating.\r\n",
        "# We need to clean this field, and we are going to use the split method to do so :\r\n",
        "\r\n",
        "def split_company_name(x):\r\n",
        "  if (type(x)==str):\r\n",
        "    return x.split('\\n')[0]\r\n",
        "  else:\r\n",
        "    return 'Unknown'\r\n",
        "\r\n",
        "def clean_company_name(df):\r\n",
        "  df['Company Name'] = df['Company Name'].apply(lambda x: split_company_name(x))\r\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azHPCN80hqOr"
      },
      "source": [
        "# 'Sector' and 'Type of ownership' columns have both a value named 'Government', it will be a problem when we'll have to perform One Hot Encoding.\r\n",
        "# Let's replace this value for one of these columns:\r\n",
        "\r\n",
        "def replace_redundant_val(df):\r\n",
        "  df['Sector'].replace({'Government':'Government Sector'}, inplace=True)\r\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7GN2u9qZI1o"
      },
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M87ez9dH46ve"
      },
      "source": [
        "def add_position(df):\r\n",
        "  df['Position'] = 'Other'\r\n",
        "  searchfor = ['data', 'analyst']\r\n",
        "  df.loc[(df['Job Title'].str.lower().str.contains(searchfor[0])) & (df['Job Title'].str.lower().str.contains(searchfor[1])), 'Position'] = 'Data Analyst'\r\n",
        "  searchfor = ['data', 'engineer']\r\n",
        "  df.loc[(df['Job Title'].str.lower().str.contains(searchfor[0])) & (df['Job Title'].str.lower().str.contains(searchfor[1])), 'Position'] = 'Data Engineer'\r\n",
        "  searchfor = ['data', 'scientist']\r\n",
        "  df.loc[(df['Job Title'].str.lower().str.contains(searchfor[0])) & (df['Job Title'].str.lower().str.contains(searchfor[1])), 'Position'] = 'Data Scientist'\r\n",
        "  searchfor = ['machine learning', 'engineer']\r\n",
        "  df.loc[(df['Job Title'].str.lower().str.contains(searchfor[0])) & (df['Job Title'].str.lower().str.contains(searchfor[1])), 'Position'] = 'ML Engineer'\r\n",
        "  searchfor = ['business', 'analyst']\r\n",
        "  df.loc[(df['Job Title'].str.lower().str.contains(searchfor[0])) & (df['Job Title'].str.lower().str.contains(searchfor[1])), 'Position'] = 'BI Analyst'\r\n",
        "  searchfor = ['data', 'architect']\r\n",
        "  df.loc[(df['Job Title'].str.lower().str.contains(searchfor[0])) & (df['Job Title'].str.lower().str.contains(searchfor[1])), 'Position'] = 'Data Architect'\r\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPVJRHcByOmU"
      },
      "source": [
        "def add_experience(df):\r\n",
        "  df['Years of experience'] = df['Job Description'].apply(lambda x: find_experience(x))\r\n",
        "  df['Level of experience'] = df['Job Title'].apply(lambda x : categorize_level_by_title(x)) # This way to find the level of experience required by checking keyword in the job title is more reliable so we put it before\r\n",
        "  df.loc[df['Level of experience']=='Unknown','Level of experience'] = df.loc[df['Level of experience']=='Unknown']['Years of experience'].apply(lambda x : categorize_level_by_years(x))\r\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYN-HdJpxBxM"
      },
      "source": [
        "# Here, we are coding a function using regular expressions patterns to detect in each job description, what is the experience needed to apply to the job\r\n",
        "def find_experience(x):\r\n",
        "  pattern1 = re.compile(r'\\d+\\+?(\\s?\\.?-?\\s?|\\s?to\\s?)(\\d?\\s?years)') #maybe add \\s[oO]f\\s[eE]xperience\r\n",
        "  matches = pattern1.finditer(x)\r\n",
        "\r\n",
        "  filteredtext = ''\r\n",
        "\r\n",
        "  for match in matches:\r\n",
        "    filteredtext += (match.group(0) + ' ') \r\n",
        "\r\n",
        "  pattern2 = re.compile(r'\\d+')\r\n",
        "  matches = pattern2.finditer(filteredtext)\r\n",
        "  years_found = np.array([])\r\n",
        "  for match in matches:\r\n",
        "    if (float(match.group(0)) <= 10):\r\n",
        "      years_found = np.append(years_found, float(match.group(0)))\r\n",
        "\r\n",
        "  if (len(years_found)==0):\r\n",
        "    return 0\r\n",
        "  else:\r\n",
        "    return round(years_found.mean(),2)\r\n",
        "\r\n",
        "def categorize_level_by_years(x):\r\n",
        "  if x == 0:\r\n",
        "    return 'Unknown'\r\n",
        "  elif x <= 1.5:\r\n",
        "    return 'Junior'\r\n",
        "  elif x <= 3:\r\n",
        "    return 'Middle'\r\n",
        "  elif x <= 5:\r\n",
        "    return 'Senior'\r\n",
        "  elif x <= 10:\r\n",
        "    return 'Technical lead'\r\n",
        "\r\n",
        "def categorize_level_by_title(x):\r\n",
        "  if (x.lower().find('junior') != -1) | (x.lower().find('jr') != -1):\r\n",
        "    return 'Junior'\r\n",
        "  elif x.lower().find('middle') != -1:\r\n",
        "    return 'Middle'\r\n",
        "  elif (x.lower().find('senior') != -1) | (x.lower().find('sr') != -1):\r\n",
        "    return 'Senior'\r\n",
        "  elif (x.lower().find('technical lead') != -1) | (x.lower().find('tech lead') != -1):\r\n",
        "    return 'Technical lead'\r\n",
        "  else:\r\n",
        "    return 'Unknown'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ducLGtnLnRdq"
      },
      "source": [
        "# Regarding the location and headquarters fields, we notice there are a bunch of cities coupled with their region, and it would more handy for our machine learning training\r\n",
        "# if we selected only the region ID to define these columns. So we are also going to use a splitting function to perform this task :\r\n",
        "\r\n",
        "def split_location(x):\r\n",
        "  if (type(x)==str):\r\n",
        "    return x.split(',')[-1].strip()\r\n",
        "  else:\r\n",
        "    return 'Unknown'\r\n",
        "\r\n",
        "def redefine_locations(df):\r\n",
        "  df['Location'] = df['Location'].apply(lambda x: split_location(x))\r\n",
        "  df['Headquarters'] = df['Headquarters'].apply(lambda x: split_location(x))\r\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zC6CE5xAtIn-"
      },
      "source": [
        "# We would like to modify the Salary Estimate column which contain string values indicating the salary but with extra $ and K symbols. As we want the value in the form of a float,\r\n",
        "# we will perform a function using the regex library to extract each number values for lower and upper estimate, and finally take the mean of both. Nevertheless, one important thing to consider\r\n",
        "# is that a few rows have their salary expressed per hour, so we will also take care of this eventuality in the function :\r\n",
        "\r\n",
        "def estimate_salary(x):\r\n",
        "  if (type(x)==str):\r\n",
        "    pattern = re.compile(r'\\d+')\r\n",
        "    ph_pattern = re.compile(r'[Pp]er\\s[Hh]our')\r\n",
        "    lower_upper = np.array([])\r\n",
        "    matches = ph_pattern.finditer(x)\r\n",
        "    ph_found = sum(1 for match in matches)\r\n",
        "    matches = pattern.finditer(x)\r\n",
        "    for match in matches:\r\n",
        "      lower_upper = np.append(lower_upper, float(match.group(0)))\r\n",
        "    if ph_found == False:\r\n",
        "      return lower_upper.mean()\r\n",
        "    else:\r\n",
        "      return (lower_upper.mean() * 40 * 48)/1000 # Considering 48 working weeks a year is the average in the United States, and assuming 40 hour week, we multiply all these values together to obtain the salary per Year\r\n",
        "  else:\r\n",
        "    return np.nan\r\n",
        "\r\n",
        "def redefine_salary(df):\r\n",
        "  df['Salary Estimate'] = df['Salary Estimate'].apply(lambda x: estimate_salary(x))\r\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeOxB_3HDBOT"
      },
      "source": [
        "def label_encode(df):\r\n",
        "  df['Size'].replace({'1 to 50 employees': 1, '51 to 200 employees': 2, '201 to 500 employees': 3, '501 to 1000 employees': 4, \r\n",
        "                      '1001 to 5000 employees': 5, '5001 to 10000 employees': 6, '10000+ employees': 7}, inplace=True)\r\n",
        "  \r\n",
        "  df['Revenue'].replace({'$100 to $500 million (USD)':7, '$2 to $5 billion (USD)':10, '$50 to $100 million (USD)':6,\r\n",
        "       '$1 to $2 billion (USD)': 9, '$5 to $10 billion (USD)':11, '$1 to $5 million (USD)': 2, '$25 to $50 million (USD)':5,\r\n",
        "       '$10+ billion (USD)':12, 'Less than $1 million (USD)':1, '$10 to $25 million (USD)': 4, '$500 million to $1 billion (USD)': 8,\r\n",
        "       '$5 to $10 million (USD)': 3}, inplace=True)\r\n",
        "  \r\n",
        "  df['Level of experience'].replace({'Junior':1, 'Middle':2, 'Senior':3, 'Technical lead':4}, inplace=True)\r\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD7y5N-5Iszw"
      },
      "source": [
        "def one_hot_encode(df):\r\n",
        "  df_location = pd.get_dummies(df['Location'])\r\n",
        "  df_sector = pd.get_dummies(df['Sector'])\r\n",
        "  df_typeofownership = pd.get_dummies(df['Type of ownership'])\r\n",
        "  df_position = pd.get_dummies(df['Position'])\r\n",
        "  df = pd.concat([df, df_location, df_sector, df_typeofownership, df_position], axis=1)\r\n",
        "  df.drop(labels=['Location','Sector','Type of ownership','Position'], axis = 1, inplace=True)\r\n",
        "  return df\r\n",
        "\r\n",
        "def stat_impute_numerical_variables(df):\r\n",
        "  df['Rating'].fillna(df['Rating'].mean(), inplace=True)\r\n",
        "  df['Salary Estimate'].fillna(df['Salary Estimate'].median(), inplace=True)\r\n",
        "  return df\r\n",
        "\r\n",
        "def stat_impute_categorical_variables(df):\r\n",
        "  df['Sector'].fillna(df['Sector'].mode()[0], inplace=True)\r\n",
        "  df['Size'].fillna(df['Size'].mode()[0], inplace=True)\r\n",
        "  df['Type of ownership'].fillna(df['Type of ownership'].mode()[0], inplace=True)\r\n",
        "  return df\r\n",
        "\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "\r\n",
        "# We can predict missing values in Revenue using a prediction model based on a classification algorithm.\r\n",
        "# To do so we are going to train our model on independant features : Salary Estimate, Rating, Location, Size, Sector, Type of ownership\r\n",
        "\r\n",
        "def ml_impute_revenue(df):\r\n",
        "  scaler = MinMaxScaler()\r\n",
        "  X_train = df.loc[(df['Revenue'].isna() == False) & (df['Revenue'] != 'Unknown / Non-Applicable'),[c for c in df.columns if c not in ('Job Title', 'Job Description', 'Company Name', 'Revenue',\r\n",
        "                                                                                                                        'Headquarters', 'Founded', 'Industry', 'Competitors', 'Easy Apply',\r\n",
        "                                                                                                                        'Position', 'Years of experience', 'Level of experience', 'BI Analyst', 'Data Analyst',\r\n",
        "                                                                                                                        'Data Architect', 'Data Engineer', 'Data Scientist', 'ML Engineer', 'Other')]]\r\n",
        "  X_train = scaler.fit_transform(X_train)\r\n",
        "  y_train = df.loc[(df['Revenue'].isna() == False) & (df['Revenue'] != 'Unknown / Non-Applicable'), 'Revenue']\r\n",
        "  y_train = y_train.astype('int')\r\n",
        "  X_test = df.loc[(df['Revenue'].isna() == True) | (df['Revenue'] == 'Unknown / Non-Applicable'),[c for c in df.columns if c not in ('Job Title', 'Job Description', 'Company Name', 'Revenue',\r\n",
        "                                                                                                                          'Headquarters', 'Founded', 'Industry', 'Competitors', 'Easy Apply',\r\n",
        "                                                                                                                          'Position', 'Years of experience', 'Level of experience', 'BI Analyst', 'Data Analyst',\r\n",
        "                                                                                                                        'Data Architect', 'Data Engineer', 'Data Scientist', 'ML Engineer', 'Other')]]\r\n",
        "  X_test = scaler.transform(X_test)\r\n",
        "  log_reg = LogisticRegression(max_iter=1000)\r\n",
        "  log_reg.fit(X_train, y_train)\r\n",
        "  y_pred = log_reg.predict(X_test)\r\n",
        "  df.loc[(df['Revenue'].isna() == True) | (df['Revenue'] == 'Unknown / Non-Applicable'), 'Revenue'] = y_pred\r\n",
        "  return df\r\n",
        "\r\n",
        "# As we did to predict the missing values in Revenue, we can predict the unknown categories in Level of experience with the help of a classification model and machine learning\r\n",
        "# we are going to train our model on independant features : Salary Estimate, Position, Rating, Location, Size, Sector, Type of ownership\r\n",
        "\r\n",
        "def ml_impute_level(df):\r\n",
        "  scaler = MinMaxScaler()\r\n",
        "  X_train = df.loc[(df['Level of experience'] != 'Unknown'),[c for c in df.columns if c not in ('Job Title', 'Job Description', 'Company Name', 'Revenue',\r\n",
        "                                                                                                                        'Headquarters', 'Founded', 'Industry', 'Competitors', 'Easy Apply',\r\n",
        "                                                                                                                        'Position', 'Years of experience', 'Level of experience')]]\r\n",
        "  X_train = scaler.fit_transform(X_train)\r\n",
        "  y_train = df.loc[(df['Level of experience'] != 'Unknown'), 'Level of experience']\r\n",
        "  y_train = y_train.astype('int')\r\n",
        "  X_test = df.loc[(df['Level of experience'] == 'Unknown'),[c for c in df.columns if c not in ('Job Title', 'Job Description', 'Company Name', 'Revenue',\r\n",
        "                                                                                                                          'Headquarters', 'Founded', 'Industry', 'Competitors', 'Easy Apply',\r\n",
        "                                                                                                                          'Position', 'Years of experience', 'Level of experience')]]\r\n",
        "  X_test = scaler.transform(X_test)\r\n",
        "  log_reg = LogisticRegression(max_iter=1000)\r\n",
        "  log_reg.fit(X_train, y_train)\r\n",
        "  y_pred = log_reg.predict(X_test)\r\n",
        "  df.loc[(df['Level of experience'] == 'Unknown'), 'Level of experience'] = y_pred\r\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aU0mZPZsTmG"
      },
      "source": [
        "def preprocess_df(df):\r\n",
        "  df = fix_df_columns(df)\r\n",
        "  df = drop_features(df)\r\n",
        "  df = replace_null(df)\r\n",
        "  df = replace_redundant_val(df)\r\n",
        "  df = convert_dtypes(df)\r\n",
        "  df = add_position(df)\r\n",
        "  df = add_experience(df)\r\n",
        "  df = clean_company_name(df)\r\n",
        "  df = redefine_locations(df)\r\n",
        "  df = redefine_salary(df)\r\n",
        "  df = stat_impute_numerical_variables(df)\r\n",
        "  df = stat_impute_categorical_variables(df)\r\n",
        "  df = label_encode(df)\r\n",
        "  df = one_hot_encode(df)\r\n",
        "  df = ml_impute_revenue(df)\r\n",
        "  df = ml_impute_level(df)\r\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEs40mB3HGil"
      },
      "source": [
        "df = pd.concat([df_dataanalyst, df_dataengineer, df_businessanalyst, df_datascientist])\r\n",
        "df = preprocess_df(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zg3aSRRIX-ym"
      },
      "source": [
        "## Selecting and Training models\r\n",
        "Now that we have completed all the preprocessing steps, we are going to use the independant features we have processed which are related to the salary to predict the salary for new inputs. The independant features (predictors) are the following : Position, Level of experience, Rating, Location, Size, Sector, Revenue, Type of ownership"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYaYHZ-Oh8B2"
      },
      "source": [
        "#X.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6HM6f9XIa6Z"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "X = df[[c for c in df.columns if c not in ('Salary Estimate','Job Title', 'Job Description','Company Name', 'Industry', 'Years of experience',\r\n",
        "                                           'Headquarters', 'Founded', 'Competitors', 'Easy Apply')]]\r\n",
        "y = df['Salary Estimate']\r\n",
        "\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\r\n",
        "\r\n",
        "scaler = MinMaxScaler()\r\n",
        "X_train = scaler.fit_transform(X_train)\r\n",
        "scaler = MinMaxScaler()\r\n",
        "X_test = scaler.fit_transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Imch2DEXpwDO",
        "outputId": "13485360-78ca-443d-ef13-45551d9f8f65"
      },
      "source": [
        "# Linear Regression\r\n",
        "\r\n",
        "from sklearn.linear_model import LinearRegression\r\n",
        "\r\n",
        "lr_reg = LinearRegression()\r\n",
        "lr_reg.fit(X_train, y_train)\r\n",
        "\r\n",
        "# Decision Tree Regressor\r\n",
        "\r\n",
        "from sklearn.tree import DecisionTreeRegressor\r\n",
        "\r\n",
        "tree_reg = DecisionTreeRegressor()\r\n",
        "tree_reg.fit(X_train, y_train)\r\n",
        "\r\n",
        "# Random Forest Regressor\r\n",
        "\r\n",
        "from sklearn.ensemble import RandomForestRegressor\r\n",
        "\r\n",
        "rf_reg = RandomForestRegressor()\r\n",
        "rf_reg.fit(X_train, y_train)\r\n",
        "\r\n",
        "# Gradient Boosting Regressor\r\n",
        "\r\n",
        "from sklearn.ensemble import GradientBoostingRegressor\r\n",
        "\r\n",
        "gbt_reg = GradientBoostingRegressor()\r\n",
        "gbt_reg.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
              "                          init=None, learning_rate=0.1, loss='ls', max_depth=3,\n",
              "                          max_features=None, max_leaf_nodes=None,\n",
              "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                          min_samples_leaf=1, min_samples_split=2,\n",
              "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                          n_iter_no_change=None, presort='deprecated',\n",
              "                          random_state=None, subsample=1.0, tol=0.0001,\n",
              "                          validation_fraction=0.1, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7j4bCYOYj3Hv"
      },
      "source": [
        "## Model Evaluation using Cross Validation - Before Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3dJB1BAj5xa",
        "outputId": "521c9160-6753-4c30-bd12-c34fb1beacf3"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\r\n",
        "\r\n",
        "scores = cross_val_score(lr_reg, X_test, y_test, scoring=\"neg_mean_squared_error\", cv = 10)\r\n",
        "print(\"Linear Regression : \", scores)\r\n",
        "\r\n",
        "scores = cross_val_score(tree_reg, X_test, y_test, scoring=\"neg_mean_squared_error\", cv = 10)\r\n",
        "print(\"Regression Tree : \", scores)\r\n",
        "\r\n",
        "scores = cross_val_score(rf_reg, X_test, y_test, scoring=\"neg_mean_squared_error\", cv = 10)\r\n",
        "print(\"Random Forest Regressor : \", scores)\r\n",
        "\r\n",
        "scores = cross_val_score(gbt_reg, X_test, y_test, scoring=\"neg_mean_squared_error\", cv = 10)\r\n",
        "print(\"Gradient Boosting Regressor : \", scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear Regression :  [-9.04339448e+02 -6.81059053e+02 -7.79614258e+02 -8.89470911e+02\n",
            " -2.60703635e+21 -7.88413050e+02 -9.15053035e+02 -6.82100295e+22\n",
            " -9.93760380e+02 -9.18973857e+02]\n",
            "Regression Tree :  [-1603.40897658 -1395.88811022 -1721.78486909 -1579.53012931\n",
            " -1861.66710163 -1469.11543745 -1750.69792914 -1786.85221608\n",
            " -1854.42148533 -1623.83356373]\n",
            "Random Forest Regressor :  [-1035.10242162  -835.80208572 -1051.12645214  -992.84061653\n",
            " -1054.35840821  -957.59059659 -1153.07038146 -1234.70306532\n",
            " -1184.48001253  -991.70579737]\n",
            "Gradient Boosting Regressor :  [ -892.51228044  -673.78258694  -789.98043482  -839.52712724\n",
            "  -860.16437312  -815.27918933  -910.1273307  -1078.2228459\n",
            "  -955.77388246  -905.01637446]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tacdlAJtljs3"
      },
      "source": [
        "## Hyperparameter Tuning using GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbipvWbn0Cmn"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDBpZoHClbZs",
        "outputId": "59cf8b6a-ec8e-4295-8668-59be4a677050"
      },
      "source": [
        "parameters = {'fit_intercept': [True, False],               \r\n",
        "              'normalize':[True, False],\r\n",
        "              'n_jobs' : [1, 10, 50, 100]}\r\n",
        "# Linear Regression\r\n",
        "lr_reg1 = GridSearchCV(LinearRegression(), parameters, cv=5, scoring='neg_mean_squared_error') \r\n",
        "lr_reg1.fit(X_train, y_train)\r\n",
        "print(\"The best parameters obtained by CV:\", lr_reg1.best_params_)\r\n",
        "print(\"The best score obtained by CV = {:5.3f}\".format(lr_reg1.best_score_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The best parameters obtained by CV: {'fit_intercept': False, 'n_jobs': 1, 'normalize': True}\n",
            "The best score obtained by CV = -359245240567123738624.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ium0RLsnVS3",
        "outputId": "5ed4c223-02bd-4e16-a021-9c771be12345"
      },
      "source": [
        "parameters = {'criterion': ['mse', 'friedman_mse'],               \r\n",
        "              'splitter':['best', 'random'],\r\n",
        "              'max_features': ['auto', 'sqrt', 'log2']}\r\n",
        "# Decision Tree Regressor\r\n",
        "tree_reg2 = GridSearchCV(DecisionTreeRegressor(), parameters, cv=5, scoring='neg_mean_squared_error') \r\n",
        "tree_reg2.fit(X_train, y_train)\r\n",
        "print(\"The best parameters obtained by CV:\", tree_reg2.best_params_)\r\n",
        "print(\"The best score obtained by CV = {:5.3f}\".format(tree_reg2.best_score_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The best parameters obtained by CV: {'criterion': 'friedman_mse', 'max_features': 'log2', 'splitter': 'random'}\n",
            "The best score obtained by CV = -1363.583\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSuGDTzTqcip",
        "outputId": "378b756c-079f-4dba-9def-d5d2c68dee1a"
      },
      "source": [
        "parameters = {'n_estimators': [100, 150, 200],               \r\n",
        "              'max_features' : ['auto', 'log2']}\r\n",
        "# Random Forest Regressor\r\n",
        "rf_reg3 = GridSearchCV(RandomForestRegressor(), parameters, cv=5, scoring='neg_mean_squared_error') \r\n",
        "rf_reg3.fit(X_train, y_train)\r\n",
        "print(\"The best parameters obtained by CV:\", rf_reg3.best_params_)\r\n",
        "print(\"The best score obtained by CV = {:5.3f}\".format(rf_reg3.best_score_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The best parameters obtained by CV: {'max_features': 'log2', 'n_estimators': 200}\n",
            "The best score obtained by CV = -955.291\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oa8c4GJr_m1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "109116b0-fcf0-41eb-fefa-9635e6cc40b0"
      },
      "source": [
        "parameters = {'learning_rate': [0.05, 0.1, 0.2, 0.3],\r\n",
        "              'n_estimators': [100, 150, 200],\r\n",
        "              'criterion': ['friedman_mse', 'mse'],\r\n",
        "              'max_features' : ['auto', 'sqrt', 'log2']}\r\n",
        "# Gradient Boosting Regressor\r\n",
        "gbt_reg4 = GridSearchCV(GradientBoostingRegressor(), parameters, cv=5, scoring='neg_mean_squared_error')\r\n",
        "gbt_reg4.fit(X_train, y_train)\r\n",
        "print(\"The best parameters by CV:\", gbt_reg4.best_params_)\r\n",
        "print(\"The best score by CV = {:5.3f}\".format(gbt_reg4.best_score_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The best parameters by CV: {'criterion': 'mse', 'learning_rate': 0.1, 'max_features': 'auto', 'n_estimators': 150}\n",
            "The best score by CV = -796.315\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-1Vm4c8SO1k"
      },
      "source": [
        "## Save the best model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ZT2xy0Q42Sbp",
        "outputId": "46afa0f2-759d-4ab3-dd1b-0ad5f1a3e9f3"
      },
      "source": [
        "import sklearn\r\n",
        "sklearn.__version__\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0.22.2.post1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4N1znhOSU21"
      },
      "source": [
        "import pickle\r\n",
        "\r\n",
        "##saving the model\r\n",
        "with open(\"gbt_model.pkl\", 'wb') as f_out:\r\n",
        "    pickle.dump(gbt_reg4, f_out)\r\n",
        "    f_out.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flDoD9myWKWt"
      },
      "source": [
        "## Save the test scaler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFnJ6y4kWRzC"
      },
      "source": [
        "##saving the model\r\n",
        "with open(\"test_scaler.pkl\", 'wb') as f_out:\r\n",
        "    pickle.dump(scaler, f_out)\r\n",
        "    f_out.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}